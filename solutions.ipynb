{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/digantasarmah1001-crypto/Risk-management/blob/main/solutions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-86e0de040aac317a",
          "locked": true,
          "schema_version": 2,
          "solution": false
        },
        "id": "1eJBpmPmCcnV"
      },
      "source": [
        "## Midterm test and practice session"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_bCRt2KCcnW"
      },
      "source": [
        "### 1. Questions.\n",
        "Please, answer the following questions briefly. Two or three sentences with main idea would be enough.\n",
        "\n",
        "Do not use external resourses in this part, please. Answer with you own words. If you forgot something, don't worry, we will discuss it later."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J01l6n5ICcnW"
      },
      "source": [
        "#### 1.0.\n",
        "Please, formulate the supervised learning problem statement.\n",
        "\n",
        "_Your answer here_\n",
        "\n",
        "\n",
        "The supervised learning problem is defined as a search for a function say\n",
        "$f: \\mathcal{X} \\rightarrow \\mathcal{Y}$, within a space say $\\mathcal{\\theta}$ . This minimizes the expected loss over the joint probability function $P(X, Y)$.\n",
        "\n",
        "Say we are given a dataset $D = \\{(x_i, y_i)\\}_{i=1}^n$ , these data set is assumed to be i.i.d and drawn from $P(X, Y)$; Our main goal is to minimize the true risk of loss which is given by the function  $R(f)$.\n",
        "\n",
        "However, because the joint distribution $P(X, Y)$ is unknown, the true risk cannot be computed directly. Instead, learning algorithms minimize the expected Risk of Loss $R_{expected}(f)$, which is the average loss on the observable training data.\n",
        "\n",
        "This is specifically called the Expeced Risk Management (ERM) and is the supervised learning model's inherent statement"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FKuUncuMCcnW"
      },
      "source": [
        "#### 1.1.\n",
        "\n",
        "What are regression and classification problems. What’s the difference?\n",
        "\n",
        "_Your answer here_\n",
        "\n",
        "In regression problems we have the task to Predict a continuous numerical value and the output that we get is a quantity (e.g., price, height, temperature), we are basically approximating a function\n",
        "\n",
        "in classification problems we are tasked to predict a discrete class label and our resultant output is a category (e.g., Spam/Not Spam) or probability. Here we find a decision boundary separating classes.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NXp2Mr4wJ2cL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YOSP8Io0CcnX"
      },
      "source": [
        "#### 1.2.\n",
        "Write down the LINEAR model for regression problem in matrix notation. What is Mean Squared Error (MSE) loss function? How can it be expressed?\n",
        "\n",
        "_Your answer here_\n",
        "\n",
        "The function given by $$\\hat{y} = w^T x$$ is usually reffered to as the linear model of a regression problem.\n",
        "\n",
        "here our vectors $w, x$ represent\n",
        "\n",
        "$$w= weight$$\n",
        "$$x= input$$\n",
        "\n",
        "\n",
        "The objective is to minimize the Mean Squared Error (MSE) loss function, which under the assumption of Gaussian errors is provided by the loss function:\n",
        "\n",
        "$$L(w) = \\frac{1}{2n} ||Xw - y||_2^2 = \\frac{1}{2n} \\sum_{i=1}^n (w^T x_i - y_i)^2$$\n",
        "\n",
        "$\\frac{1}{2}$ is simply there to make it easy to calculate in its differentiation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yVjcpe2BCcnX"
      },
      "source": [
        "#### 1.3.\n",
        "What is the difference between parameters and hyperparameters? Provide an example for linear models and decision trees.\n",
        "\n",
        "_Your answer here_\n",
        "\n",
        "Parameters are Internal variables learned directly from data and Hyperparameters are External configurations set before training.\n",
        "\n",
        "Parameters are optimized to minimize training risk. Hyperparameters are tuned on a validation set to optimize generalization.\n",
        "\n",
        "For linear models say The Weights (coefficients, $w$) and Bias (intercept, $b$) These are the values the gradient descent algorithm updates to fit the line. These are parameters\n",
        "\n",
        "For decision trees say the Maximum Depth (max_depth), Minimum Samples per Leaf are called hyper parameters\n",
        "\n",
        "a point to be noted is that both linear models and decision trees have parameters and hyperparameters\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fc3VOWYrCcnX"
      },
      "source": [
        "#### 1.4.\n",
        "Write down gradient descent step for linear model and MSE for one-dimensional case.\n",
        "\n",
        "_Your answer here_\n",
        "\n",
        "Model:\n",
        "$$\n",
        "\\hat{y}_i = w x_i + b\n",
        "$$\n",
        "\n",
        "Mean squared error:\n",
        "$$\n",
        "L(w,b) = \\frac{1}{n} \\sum_{i=1}^{n} \\big(y_i - (w x_i + b)\\big)^2\n",
        "$$\n",
        "\n",
        "Gradients:\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial w} = -\\frac{2}{n} \\sum_{i=1}^{n} x_i \\big(y_i - (w x_i + b)\\big),\n",
        "\\qquad\n",
        "\\frac{\\partial L}{\\partial b} = -\\frac{2}{n} \\sum_{i=1}^{n} \\big(y_i - (w x_i + b)\\big)\n",
        "$$\n",
        "\n",
        "Gradient descent:\n",
        "$$\n",
        "w^{(t+1)} = w^{(t)} - \\eta \\,\\frac{\\partial L}{\\partial w},\n",
        "\\qquad\n",
        "b^{(t+1)} = b^{(t)} - \\eta \\,\\frac{\\partial L}{\\partial b}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mhyJKavtCcnX"
      },
      "source": [
        "#### 1.5.\n",
        "What is validation? Cross validation? How is validation different from evaluation/test phase.\n",
        "\n",
        "_Your answer here_\n",
        "\n",
        "Validation:\n",
        "Validation is the process of using a held-out subset of the training data to tune hyperparameters, this is done so as ti prevent overfitting, and select the best model configuration.\n",
        "\n",
        "Cross-validation: Cross-validation partitions the dataset into many folds (say $k$ ). The model is trained on $k-1$ folds and validated on the remaining fold. This process repeats $k$ times, and the average validation performance is used to estimate generalization.\n",
        "\n",
        "Validation differs from Evaluation because Validation is used during training for model selection and hyperparameter tuning whereas Evaluation is performed after training is complete, on unseen data, to assess final generalization performance.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WpxkBBRiCcnX"
      },
      "source": [
        "#### 1.6.\n",
        "What is regularization? How does L1 regularization differ from L2 for linear models?\n",
        "\n",
        "_Your answer here_\n",
        "\n",
        "Regularization is just Constraining the solution by adding a penalty term $\\Omega(w)$:\n",
        "$$\n",
        "L_{reg}(w) = L(w) + \\lambda \\Omega(w)\n",
        "$$\n",
        "L1 regularization : $\\Omega(w) = ||w||_1 = \\sum |w_j|$\n",
        "\n",
        "L2 regularization :$\\Omega(w) = ||w||_2^2 = \\sum w_j^2$\n",
        "\n",
        "L1 induces sparsity (feature selection); L2 shrinks weights diffusely.\n",
        "\n",
        "Geometrically, the L1 constraint region is a diamond with corners on the axes. Loss contours are statistically likely to touch these corners, setting coefficients to exactly zero. L2 is a sphere, which contours touch tangentially, rarely at zero."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJ9KBHXACcnX"
      },
      "source": [
        "#### 1.7.\n",
        "Why is it a good idea to normalize data before applying a linear model? What kinds of data normalization do you know?\n",
        "\n",
        "_Your answer here_\n",
        "\n",
        "Normalization i the Scaling of features to a standard range (e.g., $\\mu=0, \\sigma=1$) before training. it is done because Unnormalized data creates loss contours which have ill-conditioned curvature. In L1/L2 regulariazation the penalties assume all weights operate on the same scale. Without normalization, features with small units (e.g., mm vs km) acquire large weights and are unfairly penalized.\n",
        "\n",
        "A few types of normalization are-\n",
        "\n",
        "Z-Score Standardization\n",
        "$$\n",
        "x' = \\frac{x - \\mu}{\\sigma}\n",
        "$$\n",
        "Centers data at mean 0 with unit variance.\n",
        "\n",
        "Log Transformation\n",
        "$$\n",
        "x' = \\log(1+x)\n",
        "$$\n",
        "Useful for skewed distributions, compresses large values."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZHQ0fj5QCcnY"
      },
      "source": [
        "#### 1.8.\n",
        "What are precision and recall metrics? Explain how you understand their meaning.\n",
        "\n",
        "_Your answer here_\n",
        "\n",
        "Precision is defined as Exactness or how many predicted positives are actually positive .\n",
        "Recall is defined as Completeness or how many actual positives did we find.\n",
        "\n",
        "Accuracy is useless when classes are imbalanced. We optimize Precision when False Positives are costly. We optimize Recall when False Negatives are costly. The trade-off is often managed via the F1-Score."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eAcyOWA4CcnY"
      },
      "source": [
        "#### 1.9.\n",
        "How Random Forest is different from simple bagging?\n",
        "\n",
        "_Your answer here_\n",
        "\n",
        "Random Forest is Bagging plus Feature Subsampling. At every split, RF considers only a random subset of features. In standard Bagging, trees remain correlated because they all split on the same dominant features. Feature subsampling forces trees to be diverse (uncorrelated). Averaging uncorrelated trees reduces variance much more effectively than averaging correlated trees."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9B1A9WRmCcnY"
      },
      "source": [
        "#### 1.10\n",
        "\n",
        "How are base algorithms being trained in gradient boosting? Do we need to apply data normalization for gradient boosting?\n",
        "\n",
        "_Your answer here_\n",
        "\n",
        "Gradient boosting builds an additive model:\n",
        "\n",
        "$$F_{m}(x) = F_{m-1}(x) + \\nu \\cdot h_m(x),\n",
        "$$\n",
        "\n",
        "At each stage $m$: Pseudo residuals are calculated, then fit with the base learner and then whole ensemble updates\n",
        "\n",
        "No, gradient boosting does not require feature normalization this is because decision trees split based on thresholds of raw feature values.They are invariant to monotonic transformations and scaling of features.Unlike linear models or gradient descent–based algorithms, normalization does not affect tree splits."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5co18zZsCcnY"
      },
      "source": [
        "### 2. Optional discussion\n",
        "\n",
        "It is optional. If you want to share some suggestions or comments, use this area. We will contact you in private messages if needed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PT3flEr5CcnY"
      },
      "source": []
    }
  ],
  "metadata": {
    "celltoolbar": "Create Assignment",
    "kernelspec": {
      "display_name": "Py3 research env",
      "language": "python",
      "name": "py3_research"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": false,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}